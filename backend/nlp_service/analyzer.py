# nlp_service/analyzer.py 
import spacy
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer, util
import numpy as np
import re
from typing import Dict, List, Tuple
import logging
import torch
import math
import pandas as pd
import os
import joblib

# Configuration du logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MLCVAnalyzer:
    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        try:
            logger.info("Initialisation de l'analyseur de CV...")
            
            # Charger le mod√®le ML pr√©-entra√Æn√©
            self.ml_matcher = self._load_ml_model()
            
            # Charger spaCy pour le fran√ßais
            try:
                self.nlp = spacy.load("fr_core_news_sm")
                logger.info("‚úÖ Mod√®le spaCy fran√ßais charg√©")
            except OSError:
                logger.warning("spaCy non disponible, utilisation de m√©thodes simples")
                self.nlp = None
            
            # Charger les comp√©tences depuis le dataset
            self._skills_set = self._load_skills_from_dataset()
            logger.info(f"‚úÖ {len(self._skills_set)} comp√©tences charg√©es depuis le dataset")
                
        except Exception as e:
            logger.error(f"‚ùå Erreur critique lors de l'initialisation: {e}")
            raise

    def _load_ml_model(self):
        """Charge le mod√®le ML pr√©-entra√Æn√© depuis train_model.py"""
        try:
            # Chercher le mod√®le dans diff√©rents chemins
            model_paths = [
                'models/cv_job_matcher.pkl',
                os.path.join(os.path.dirname(__file__), 'models/cv_job_matcher.pkl'),
                os.path.join(os.path.dirname(os.path.dirname(__file__)), 'models/cv_job_matcher.pkl'),
            ]
            
            model_path = None
            for path in model_paths:
                if os.path.exists(path):
                    model_path = path
                    break
            
            if model_path:
                # Importer dynamiquement la classe CVJobMatcher
                # Import from the same package
                from .train_model import CVJobMatcher
                ml_model = CVJobMatcher.load_model(model_path)
                logger.info("‚úÖ Mod√®le ML charg√© avec succ√®s")
                return ml_model
            else:
                logger.warning("‚ùå Mod√®le ML non trouv√©, utilisation de l'analyse basique")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le ML: {e}")
            return None

    def _load_skills_from_dataset(self) -> set:
        """Charge les comp√©tences depuis le dataset UpdatedResumeDataSet.csv"""
        skills_set = set()
        
        try:
            # Chercher le dataset
            dataset_paths = [
                os.path.join(os.path.dirname(__file__), 'UpdatedResumeDataSet.csv'),
                os.path.join(os.path.dirname(os.path.dirname(__file__)), 'UpdatedResumeDataSet.csv'),
                os.path.join(os.getcwd(), 'UpdatedResumeDataSet.csv'),
            ]
            
            dataset_path = None
            for path in dataset_paths:
                if os.path.exists(path):
                    dataset_path = path
                    break
            
            if not dataset_path:
                logger.warning("Dataset non trouv√©")
                return set()
            
            # Lire le dataset
            df = pd.read_csv(dataset_path, encoding='latin-1')
            logger.info(f"Dataset charg√©: {len(df)} entr√©es")
            
            # Extraire les comp√©tences de chaque CV
            for idx, row in df.iterrows():
                resume_text = str(row['Resume']).lower()
                skills_from_cv = self._extract_skills_from_resume_text(resume_text)
                skills_set.update(skills_from_cv)
            
            # Filtrer les comp√©tences valides
            skills_set = {skill for skill in skills_set if self._is_valid_skill(skill)}
            logger.info(f"Comp√©tences extraites: {len(skills_set)}")
            
            return skills_set
            
        except Exception as e:
            logger.error(f"Erreur lors du chargement du dataset: {e}")
            return set()

    def _extract_skills_from_resume_text(self, text: str) -> set:
        """Extrait les comp√©tences d'un texte de CV"""
        skills = set()
        if not text:
            return skills
        
        text = re.sub(r'\s+', ' ', text).strip().lower()
        
        # Patterns pour sections de comp√©tences
        skill_patterns = [
            r'(?:skills?|technical skills|programming skills|technologies?|tools)[:\s\-]*(.*?)(?:\n\n|\n[A-Z]|\n\s*$|$)',
            r'(?:languages?|programming languages|langages?)[:\s\-]*(.*?)(?:\n\n|\n[A-Z]|\n\s*$|$)',
        ]
        
        # Chercher dans les sections
        for pattern in skill_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            for section in matches:
                # S√©parer les comp√©tences
                parts = re.split(r'[,;‚Ä¢\-\n]', section)
                for part in parts:
                    skill = self._clean_skill(part)
                    if skill:
                        skills.add(skill)
        
        return skills

    def _clean_skill(self, skill_text: str) -> str:
        """Nettoie une comp√©tence"""
        if not skill_text:
            return ""
        
        skill = skill_text.strip()
        skill = re.sub(r'[^\w\s\+#\.\/]', ' ', skill)
        skill = re.sub(r'\s+', ' ', skill).strip()
        skill = skill.lower()
        
        return skill

    def _is_valid_skill(self, skill: str) -> bool:
        """V√©rifie si une comp√©tence est valide"""
        if not skill or len(skill) < 2:
            return False
        
        excluded_words = {
            'and', 'or', 'the', 'a', 'an', 'with', 'using', 'knowledge', 'experience',
            'good', 'strong', 'excellent', 'basic', 'advanced'
        }
        
        if skill in excluded_words:
            return False
        
        if len(skill) > 50:
            return False
        
        return True

    def extract_text_from_pdf(self, pdf_file) -> str:
        """Extrait le texte d'un PDF avec gestion am√©lior√©e des erreurs et formats"""
        text = ""
        
        try:
            # Gestion du fichier (fichier upload√© ou chemin)
            file_obj = None
            if hasattr(pdf_file, 'read'):
                # Si c'est un fichier upload√©, on le r√©initialise
                pdf_file.seek(0)
                file_obj = pdf_file
                reader = PdfReader(file_obj)
            elif isinstance(pdf_file, str) and os.path.exists(pdf_file):
                # Si c'est un chemin de fichier
                with open(pdf_file, 'rb') as f:
                    reader = PdfReader(f)
            else:
                # Si c'est d√©j√† un objet PdfReader ou similaire
                reader = pdf_file
            
            # Essayer d'extraire le texte de chaque page
            for page in reader.pages:
                try:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                except Exception as page_error:
                    logger.warning(f"Erreur extraction page: {page_error}")
                    continue
            
            # Si aucun texte n'a √©t√© extrait, essayer une m√©thode alternative
            if not text.strip():
                logger.warning("Aucun texte extrait avec extract_text(), tentative avec extract_text(0)")
                for page in reader.pages:
                    try:
                        # Certains PDF n√©cessitent extract_text(0) au lieu de extract_text()
                        page_text = page.extract_text(0)  # Mode 0 pour une extraction plus agressive
                        if page_text and len(page_text) > 10:  # V√©rifier que le texte a une longueur minimale
                            text += page_text + "\n"
                    except Exception as alt_error:
                        logger.warning(f"Erreur extraction alternative: {alt_error}")
                        continue
            
            # Nettoyer le texte extrait
            text = self._clean_extracted_text(text)
            
            if not text.strip():
                logger.warning("Aucun texte valide extrait apr√®s nettoyage")
                return ""
                
            logger.info(f"Texte PDF extrait: {len(text)} caract√®res")
            return text
            
        except Exception as e:
            logger.error(f"Erreur critique extraction PDF: {e}", exc_info=True)
            # Essayer une derni√®re m√©thode de secours
            try:
                import io
                import PyPDF2
                
                if hasattr(pdf_file, 'read'):
                    pdf_file.seek(0)
                    pdf_data = pdf_file.read()
                elif isinstance(pdf_file, str) and os.path.exists(pdf_file):
                    with open(pdf_file, 'rb') as f:
                        pdf_data = f.read()
                else:
                    return ""
                
                # Essayer avec un nouvel objet PdfReader
                with io.BytesIO(pdf_data) as f:
                    reader = PyPDF2.PdfReader(f)
                    for page in reader.pages:
                        text += page.extract_text() + "\n"
                
                text = self._clean_extracted_text(text)
                if text.strip():
                    logger.info(f"Texte extrait avec m√©thode de secours: {len(text)} caract√®res")
                    return text
                
            except Exception as fallback_error:
                logger.error(f"√âchec de la m√©thode de secours: {fallback_error}")
            
            return ""
    
    def _clean_extracted_text(self, text: str) -> str:
        """Nettoie le texte extrait du PDF"""
        if not text:
            return ""
        
        # Remplacer les sauts de ligne multiples par un seul espace
        text = re.sub(r'\s+', ' ', text)
        
        # Supprimer les caract√®res non imprimables
        text = ''.join(char for char in text if char.isprintable() or char.isspace())
        
        # Supprimer les espaces multiples
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def _clean_text(self, text: str) -> str:
        """Nettoie le texte pour l'analyse"""
        if not text:
            return ""
        
        text = text.lower()
        text = re.sub(r'[^a-zA-Z0-9√†√¢√§√©√®√™√´√Æ√Ø√¥√∂√π√ª√º√ø√ß√Ä√Ç√Ñ√â√à√ä√ã√é√è√î√ñ√ô√õ√ú≈∏√á\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text

    def extract_skills(self, text: str) -> Dict[str, float]:
        """Extrait les comp√©tences techniques"""
        if not text or not text.strip():
            return {}
        
        if not self._skills_set:
            return {}
        
        text_lower = text.lower()
        skills_found = {}
        
        for skill in self._skills_set:
            if skill in text_lower:
                count = text_lower.count(skill)
                if count > 0:
                    weight = 1 + math.log(count)
                    skills_found[skill] = weight
        
        # Normaliser
        if skills_found:
            max_weight = max(skills_found.values())
            skills_found = {k: v/max_weight for k, v in skills_found.items()}
        
        return skills_found

    def extract_experience_years(self, text: str) -> int:
        """Extrait les ann√©es d'exp√©rience"""
        if not text:
            return 0
            
        text_lower = text.lower()
        
        patterns = [
            r'(\d+)\s*(?:ans?|ann√©es?)\s+(?:d\'?exp√©rience|d\'?exp)',
            r'exp√©rience\s*[\-:]\s*(\d+)\s*(?:ans?|ann√©es?)',
            r'(\d+)\s*(?:years?|ans?|ann√©es?)(?:\s+d\'exp√©rience|\s+exp√©rience|\s+of\s+experience)?',
            r'exp√©rience\s*:\s*(\d+\+?)\s*(?:ans?|ann√©es?)',
        ]
        
        years_found = []
        
        for pattern in patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]  # Prendre le premier groupe captur√©
                try:
                    years = int(''.join(filter(str.isdigit, str(match))))
                    years_found.append(years)
                except (ValueError, TypeError):
                    continue
        
        return max(years_found) if years_found else 0

    def calculate_compatibility(self, cv_text: str, job_description: str, pdf_file=None) -> Tuple[float, List[str], List[str]]:
        """Calcule la compatibilit√© entre CV et offre"""
        logger.info("üéØ D√©but du calcul de compatibilit√©")
        
        if not cv_text or not job_description:
            logger.warning("Texte CV ou description d'emploi manquant")
            return 0.0, [], []
        
        try:
            # Stocker la r√©f√©rence au fichier PDF pour une √©ventuelle r√©extraction
            if pdf_file:
                self.last_pdf_file = pdf_file
            
            # 1. Nettoyage initial du texte
            cv_clean = self._clean_text(cv_text)
            job_clean = self._clean_text(job_description)
            
            # V√©rifier si le texte extrait est trop court ou semble invalide
            if len(cv_clean.split()) < 10:  # Moins de 10 mots
                logger.warning("Le texte extrait du CV est trop court pour une analyse fiable")
                
                # Essayer d'extraire √† nouveau avec une m√©thode diff√©rente si possible
                if hasattr(self, 'last_pdf_file') and self.last_pdf_file:
                    logger.info("Tentative d'extraction alternative...")
                    alt_text = self.extract_text_from_pdf(self.last_pdf_file)
                    if alt_text and len(alt_text.split()) >= 10:
                        cv_clean = self._clean_text(alt_text)
                        logger.info(f"Extraction alternative r√©ussie: {len(alt_text)} caract√®res")
                    else:
                        logger.warning("L'extraction alternative n'a pas fourni de texte valide")
                
                # Si toujours pas de contenu valide, retourner un score bas mais pas nul
                if len(cv_clean.split()) < 10:
                    logger.warning("Texte CV insuffisant, utilisation d'un score minimal")
                    job_skills = self.extract_skills(job_clean)
                    return 15.0, [], list(job_skills.keys()) if job_skills else []
            
            # 2. Utiliser le mod√®le ML si disponible (60% du score)
            ml_score = 0
            if self.ml_matcher:
                try:
                    ml_score = self.ml_matcher.calculate_match_score(cv_clean, job_clean)
                    logger.info(f"ü§ñ Score ML: {ml_score}%")
                    
                    # Si le score ML est tr√®s bas, v√©rifier si c'est d√ª √† une mauvaise extraction
                    if ml_score < 10 and hasattr(self, 'last_pdf_file') and self.last_pdf_file:
                        logger.warning("Score ML tr√®s bas, v√©rification de l'extraction...")
                        alt_text = self.extract_text_from_pdf(self.last_pdf_file)
                        if alt_text and len(alt_text.split()) > len(cv_clean.split()) * 1.5:  # 50% plus de contenu
                            logger.info("Meilleur texte trouv√©, r√©essai avec le nouveau contenu")
                            return self.calculate_compatibility(alt_text, job_clean, self.last_pdf_file)
                except Exception as e:
                    logger.warning(f"Erreur mod√®le ML: {e}")
                    ml_score = 0
            
            # 3. Analyse basique avec le dataset (40% du score)
            cv_skills = self.extract_skills(cv_clean)
            job_skills = self.extract_skills(job_clean)
            
            matched_skills = []
            missing_skills = []
            basic_score = 0
            
            if job_skills:
                for skill, job_weight in job_skills.items():
                    if skill in cv_skills:
                        cv_weight = cv_skills[skill]
                        match_score = (job_weight + cv_weight) / 2
                        basic_score += match_score * 40 / len(job_skills)
                        matched_skills.append(skill)
                    else:
                        missing_skills.append(skill)
            else:
                basic_score = 12  # Score minimal
            
            # 4. Score final combin√© avec pond√©ration
            final_score = ml_score * 0.6 + basic_score
            
            # Ajustement bas√© sur la longueur du texte (p√©nalit√© pour les textes courts)
            word_count = len(cv_clean.split())
            if word_count < 50:  # Moins de 50 mots
                length_penalty = 0.5 + (word_count / 100)  # 50% √† 100% du score
                final_score *= length_penalty
                logger.info(f"Ajustement pour texte court: {length_penalty:.2f}x")
            
            # S'assurer que le score est dans une plage raisonnable
            final_score = max(0, min(100, final_score))
            
            # Ajouter une petite variation pour √©viter les ex-aequo
            import hashlib
            content_hash = int(hashlib.md5(cv_clean.encode()).hexdigest()[:8], 16)
            variation = (content_hash % 100) * 0.01  # Variation de 0 √† 1%
            final_score += variation
            final_score = round(final_score, 2)
            
            logger.info(f"üéØ Score final: {final_score}% (ML: {ml_score}%, Basique: {basic_score}%)")
            
            return final_score, matched_skills, missing_skills
            
        except Exception as e:
            logger.error(f"Erreur dans calculate_compatibility: {e}", exc_info=True)
            # Retourner un score minimal plut√¥t que 0 pour √©viter de p√©naliser trop fortement
            job_skills = self.extract_skills(job_description) if job_description else {}
            return 10.0, [], list(job_skills.keys())

    def analyze(self, cv_text: str, job_description: str, pdf_file=None) -> Dict:
        """
        Analyse compl√®te d'un CV par rapport √† une offre
        
        Args:
            cv_text: Texte extrait du CV
            job_description: Description du poste
            pdf_file: Fichier PDF optionnel pour r√©extraction si n√©cessaire
            
        Returns:
            Dictionnaire contenant les r√©sultats de l'analyse
        """
        try:
            # Calcul du score de compatibilit√©
            score, matched, missing = self.calculate_compatibility(cv_text, job_description, pdf_file)
            
            # Pr√©diction de la cat√©gorie avec ML
            try:
                category, confidence = self.predict_job_category(cv_text)
            except Exception as e:
                logger.error(f"Erreur pr√©diction cat√©gorie: {e}")
                category, confidence = "Non d√©termin√©", 0.0
            
            # Extraction des comp√©tences
            try:
                cv_skills = self.extract_skills(self._clean_text(cv_text))
                job_skills = self.extract_skills(self._clean_text(job_description))
            except Exception as e:
                logger.error(f"Erreur extraction comp√©tences: {e}")
                cv_skills = {}
                job_skills = {}
            
            # G√©n√©ration du r√©sum√©
            try:
                summary = self.summarize_cv(cv_text)
            except Exception as e:
                logger.error(f"Erreur g√©n√©ration r√©sum√©: {e}")
                summary = "R√©sum√© non disponible"
            
            # Construction du r√©sultat
            result = {
                'match_score': score,
                'matched_skills': matched,
                'missing_skills': missing,
                'cv_skills': cv_skills,
                'job_skills': job_skills,
                'job_category': category,
                'category_confidence': confidence,
                'analysis_summary': summary,
                'ml_model_used': self.ml_matcher is not None,
                'success': True
            }
            
            # Ajouter des m√©tadonn√©es de d√©bogage si n√©cessaire
            if score < 15:  # Si le score est tr√®s bas, ajouter des infos de d√©bogage
                result['debug_info'] = {
                    'cv_text_length': len(cv_text) if cv_text else 0,
                    'job_text_length': len(job_description) if job_description else 0,
                    'cv_word_count': len(cv_text.split()) if cv_text else 0,
                    'job_word_count': len(job_description.split()) if job_description else 0,
                    'cv_skills_count': len(cv_skills),
                    'job_skills_count': len(job_skills),
                }
            
            return result
            
        except Exception as e:
            logger.error(f"Erreur critique dans analyze: {e}", exc_info=True)
            return {
                'match_score': 0,
                'matched_skills': [],
                'missing_skills': [],
                'cv_skills': {},
                'job_skills': {},
                'job_category': 'Erreur',
                'category_confidence': 0,
                'analysis_summary': f'Erreur lors de l\'analyse: {str(e)}',
                'ml_model_used': False,
                'success': False,
                'error': str(e)
            }

    def _clean_skill(self, skill: str) -> str:
        """Nettoie une comp√©tence en supprimant les mots vides et caract√®res sp√©ciaux"""
        # Liste des mots vides √† supprimer
        stop_words = {'de', 'la', 'le', 'les', 'et', 'ou', 'avec', 'sans', 'pour', 'dans', 
                     'sur', 'sous', 'par', 'au', 'aux', 'du', 'des', 'un', 'une', 'a', 'b', 
                     'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 
                     'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'on', 'te', 'is', 
                     're', 'et', 'to', 'in', 'it', 'at'}
        
        # Supprimer les caract√®res sp√©ciaux et les chiffres
        cleaned = re.sub(r'[^\w\s]', ' ', skill.lower())
        # Supprimer les mots courts non pertinents
        words = [word for word in cleaned.split() 
                if len(word) > 2 and word.lower() not in stop_words]
        
        return ' '.join(words).strip().capitalize()

    def extract_skills(self, text: str) -> Dict[str, float]:
        """Extrait les comp√©tences techniques du texte avec un meilleur filtrage"""
        if not text or not isinstance(text, str):
            return {}

        # Nettoyer le texte
        text_clean = self._clean_text(text)
        
        # Convertir en minuscules pour la correspondance insensible √† la casse
        text_lower = text_clean.lower()
        
        # Dictionnaire pour stocker les comp√©tences trouv√©es avec leur score
        skills_found = {}
        
        # 1. V√©rifier les comp√©tences du dataset
        for skill in self._skills_set:
            # Ignorer les comp√©tences trop courtes
            if len(skill) < 3:
                continue
                
            # V√©rifier la pr√©sence de la comp√©tence dans le texte
            if skill.lower() in text_lower:
                # Calculer un score bas√© sur la longueur de la comp√©tence
                # et la fr√©quence d'apparition
                count = text_lower.count(skill.lower())
                score = min(len(skill) * 0.1 * (1 + count * 0.2), 1.0)  # Score entre 0.1 et 1.0
                skills_found[skill] = max(skills_found.get(skill, 0), score)
        
        # 2. D√©tection des langages de programmation (score √©lev√© car tr√®s sp√©cifiques)
        programming_keywords = {
            'python', 'java', 'javascript', 'typescript', 'c++', 'c#', 'php', 'ruby',
            'swift', 'kotlin', 'go', 'rust', 'scala', 'r', 'matlab', 'bash', 'sql',
            'html', 'css', 'sass', 'less', 'dart', 'perl', 'haskell', 'elixir', 'erlang'
        }
        
        # 3. D√©tection des frameworks et biblioth√®ques
        framework_keywords = {
            'django', 'flask', 'fastapi', 'spring', 'spring boot', 'react', 'angular', 
            'vue', 'vue.js', 'node.js', 'express', 'laravel', 'ruby on rails', 'asp.net',
            'tensorflow', 'pytorch', 'keras', 'pandas', 'numpy', 'scikit-learn', 'opencv',
            'react native', 'flutter', 'xamarin', 'ionic', 'electron', 'next.js', 'nuxt.js',
            'graphql', 'apollo', 'grpc', 'thrift', 'kafka', 'rabbitmq', 'celery'
        }
        
        # 4. D√©tection des outils et plateformes
        tool_keywords = {
            'git', 'github', 'gitlab', 'bitbucket', 'docker', 'kubernetes', 'jenkins',
            'ansible', 'terraform', 'aws', 'amazon web services', 'azure', 'google cloud',
            'gcp', 'postgresql', 'mysql', 'mongodb', 'redis', 'elasticsearch', 'kibana',
            'prometheus', 'grafana', 'splunk', 'datadog', 'new relic', 'sentry', 'jira',
            'confluence', 'trello', 'asana', 'slack', 'microsoft teams', 'zoom', 'figma',
            'sketch', 'adobe xd', 'zeplin', 'invision', 'docker-compose', 'helm', 'argo',
            'istio', 'linkerd', 'consul', 'vault', 'terraform cloud', 'pulumi', 'serverless'
        }
        
        # 5. V√©rifier les mots-cl√©s dans le texte avec des scores diff√©rents
        for keyword_set, base_score in [
            (programming_keywords, 0.9),   # Score √©lev√© pour les langages
            (framework_keywords, 0.8),     # Score moyen-√©lev√© pour les frameworks
            (tool_keywords, 0.7)           # Score moyen pour les outils
        ]:
            for keyword in keyword_set:
                if keyword in text_lower:
                    # Si le mot-cl√© contient plusieurs mots, v√©rifier la correspondance exacte
                    if ' ' in keyword:
                        if keyword in text_lower:
                            skills_found[keyword] = max(skills_found.get(keyword, 0), base_score)
                    else:
                        # Pour les mots simples, v√©rifier les limites de mot
                        words = set(text_lower.split())
                        if keyword in words:
                            skills_found[keyword] = max(skills_found.get(keyword, 0), base_score)
        
        # 6. Utiliser spaCy pour l'extraction des entit√©s nomm√©es si disponible
        if hasattr(self, 'nlp') and self.nlp:
            try:
                doc = self.nlp(text_clean)
                for ent in doc.ents:
                    if ent.label_ in ['ORG', 'PRODUCT', 'TECH'] and len(ent.text) > 2:
                        skill = ent.text.lower().strip()
                        skills_found[skill] = max(skills_found.get(skill, 0), 0.6)
            except Exception as e:
                logger.warning(f"Erreur lors de l'extraction des entit√©s avec spaCy: {e}")
        
        # 7. Filtrer les comp√©tences trop courtes ou non pertinentes
        filtered_skills = {
            skill: score for skill, score in skills_found.items()
            if len(skill) > 2 and not any(c.isdigit() for c in skill)
        }
        
        # Trier les comp√©tences par score d√©croissant
        sorted_skills = dict(sorted(
            filtered_skills.items(),
            key=lambda item: item[1],
            reverse=True
        ))
        
        return sorted_skills

    def summarize_cv(self, cv_text: str) -> str:
        """G√©n√®re un r√©sum√© concis du CV avec des comp√©tences pertinentes"""
        if not cv_text or not isinstance(cv_text, str):
            return "Aucun contenu √† r√©sumer."
        
        try:
            # Nettoyer le texte
            clean_text = self._clean_text(cv_text)
            
            # Pr√©dire la cat√©gorie
            category, confidence = self.predict_job_category(clean_text)
            
            # Extraire les informations cl√©s
            experience_years = self.extract_experience_years(clean_text)
            
            # Extraire et nettoyer les comp√©tences
            skills = self.extract_skills(clean_text)
            
            # Filtrer et trier les comp√©tences
            filtered_skills = {}
            for skill, score in skills.items():
                cleaned_skill = self._clean_skill(skill)
                if cleaned_skill and len(cleaned_skill) > 2:  # Ignorer les mots trop courts
                    if cleaned_skill not in filtered_skills or score > filtered_skills[cleaned_skill]:
                        filtered_skills[cleaned_skill] = score
            
            # Prendre les 5 meilleures comp√©tences
            top_skills = sorted(filtered_skills.items(), 
                              key=lambda x: x[1], 
                              reverse=True)[:5]
            
            # Construire le r√©sum√©
            summary_parts = []
            
            # Ligne 1 : Cat√©gorie et exp√©rience
            category_line = f"Profil {category} avec "
            if experience_years > 1:
                category_line += f"{experience_years} ans d'exp√©rience professionnelle."
            elif experience_years == 1:
                category_line += "1 an d'exp√©rience professionnelle."
            else:
                category_line += "peu ou pas d'exp√©rience professionnelle."
            
            summary_parts.append(category_line)
            
            # Ligne 2 : Comp√©tences cl√©s
            if top_skills:
                skills_list = ", ".join([s[0] for s in top_skills if s[0].strip()])
                if skills_list:
                    summary_parts.append(f"Comp√©tences cl√©s : {skills_list}.")
            
            # Ligne 3 : Note sur la confiance
            if confidence > 0.5:  # Ne montrer que si la confiance est raisonnable
                summary_parts.append(f"(Niveau de confiance de l'analyse : {confidence*100:.0f}%)")
            
            # Retourner le tout avec des sauts de ligne
            return "\n\n".join(summary_parts) if summary_parts else "R√©sum√© non disponible"
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du r√©sum√©: {e}")
            return "R√©sum√© temporairement indisponible."

    def _extract_detailed_experience(self, text: str) -> str:
        """Extrait les informations d'exp√©rience d√©taill√©es"""
        experience_years = self.extract_experience_years(text)
        experience_info = []
        
        # D√©tection des postes occup√©s
        job_titles = self._extract_job_titles(text)
        if job_titles:
            experience_info.append(f"Postes r√©cents: {', '.join(job_titles[:3])}")
        
        # Ajout des ann√©es d'exp√©rience
        if experience_years > 0:
            experience_info.append(f"Total d'exp√©rience: {experience_years} ans")
        else:
            experience_info.append("D√©butant ou exp√©rience non sp√©cifi√©e")
        
        return ". ".join(experience_info) + "."
    
    def _extract_job_titles(self, text: str) -> list:
        """Extrait les intitul√©s de poste du CV"""
        job_titles = []
        # Expressions r√©guli√®res pour d√©tecter les intitul√©s de poste
        patterns = [
            r"(?:D√©veloppeur|Ing√©nieur|Chef de projet|Consultant|Technicien)\s+\w+",
            r"\b(?:Senior|Junior|Lead|Architecte)\s+\w+",
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            job_titles.extend([m.strip() for m in matches if len(m) > 5])
        
        return list(dict.fromkeys(job_titles))  # Supprime les doublons
    
    def _extract_education(self, text: str) -> str:
        """Extrait les informations de formation"""
        education_keywords = [
            "dipl√¥me", "licence", "master", "doctorat", "ing√©nieur", "bac", "bts", "dut",
            "formation", "√©cole", "universit√©", "√©tudes", "graduation"
        ]
        
        education = []
        lines = text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in education_keywords):
                education.append(line.strip())
        
        return "\n".join(education[:3]) if education else "Formation non sp√©cifi√©e"
    
    def _extract_projects(self, text: str) -> str:
        """Extrait les projets mentionn√©s dans le CV"""
        project_keywords = ["projet", "r√©alisation", "mission", "exp√©rience", "cas pratique"]
        projects = []
        lines = text.split('\n')
        
        for line in lines:
            if any(keyword in line.lower() for keyword in project_keywords):
                projects.append(f"- {line.strip()}")
        
        return "\n".join(projects[:3]) if projects else "Aucun projet sp√©cifi√©"

    def predict_job_category(self, text: str) -> Tuple[str, float]:
        """
        Pr√©dit la cat√©gorie d'emploi √† partir d'un texte de CV ou d'offre d'emploi
        
        Args:
            text: Texte √† analyser (CV ou offre d'emploi)
            
        Returns:
            Tuple[str, float]: (Cat√©gorie d'emploi pr√©dite, Niveau de confiance 0-1)
        """
        if not text or not isinstance(text, str):
            return "Inconnu", 0.0
            
        # Liste des cat√©gories possibles (√† adapter selon vos besoins)
        categories = [
            "D√©veloppement", "R√©seau et s√©curit√©", "Data Science", 
            "DevOps", "Design", "Marketing", "Ventes", "Ressources Humaines"
        ]
        
        # Si le mod√®le ML est disponible, l'utiliser pour la pr√©diction
        if hasattr(self, 'ml_matcher') and self.ml_matcher is not None:
            try:
                # Utiliser le mod√®le pour pr√©dire la cat√©gorie
                category, confidence = self.ml_matcher.predict_category(text)
                if category:
                    return str(category), float(confidence)
            except Exception as e:
                logger.error(f"Erreur lors de la pr√©diction de cat√©gorie: {e}")
                logger.error(f"Type d'erreur: {type(e).__name__}", exc_info=True)
        
        # M√©thode de repli bas√©e sur des mots-cl√©s avec une confiance plus faible
        text_lower = text.lower()
        if any(word in text_lower for word in ["devops", "deploy", "ci/cd", "aws", "azure", "docker", "kubernetes"]):
            return "DevOps", 0.7
        elif any(word in text_lower for word in ["data", "machine learning", "ai", "intelligence artificielle"]):
            return "Data Science", 0.7
        elif any(word in text_lower for word in ["frontend", "front-end", "react", "angular", "vue", "javascript"]):
            return "D√©veloppement Frontend", 0.7
        elif any(word in text_lower for word in ["backend", "back-end", "node", "django", "spring", ".net"]):
            return "D√©veloppement Backend", 0.7
        elif any(word in text_lower for word in ["r√©seau", "s√©curit√©", "cybers√©curit√©", "admin syst√®me"]):
            return "R√©seau et s√©curit√©", 0.7
            
        return "Autre", 0.5

    def rank_cvs(self, cvs_data: List[Dict], job_description: str) -> List[Dict]:
        """
        Classe plusieurs CVs par rapport √† une offre
        
        Args:
            cvs_data: Liste de dictionnaires [{'id': 1, 'text': '...'}, ...]
            job_description: Texte de l'offre d'emploi
            
        Returns:
            Liste tri√©e par score d√©croissant
        """
        results = []
        
        for cv_data in cvs_data:
            cv_id = cv_data.get('id')
            cv_text = cv_data.get('text', '')
            
            if not cv_text:
                continue
                
            # Analyser ce CV
            analysis = self.analyze(cv_text, job_description)
            
            results.append({
                'cv_id': cv_id,
                'match_score': analysis['match_score'],
                'job_category': analysis['job_category'],
                'category_confidence': analysis['category_confidence'],
                'matched_skills': analysis['matched_skills'],
                'missing_skills': analysis['missing_skills'],
                'analysis_summary': analysis['analysis_summary']
            })
        
        # Trier par score d√©croissant
        results.sort(key=lambda x: x['match_score'], reverse=True)
        
        return results